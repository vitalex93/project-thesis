{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avitsas\\Anaconda3\\envs\\thesisV3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ReportGenerator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Payment predictions 1': 'Provide a report that displays the number of accounts with running settlements for the Earth portfolio, their expected monthly \\\n",
    "                  payments at the end of the examined month and their actual monthly payments per DCA, for September 2020. ',\n",
    "    'Payment predictions 2': 'Provide a report that displays the number of accounts not in running settlements for the Earth portfolio, their expected monthly \\\n",
    "                  payments at the end of the examined month and their actual monthly payments per DCA, for September 2020.',\n",
    "    'Report 6_1': 'Provide the number of customers with active and running settlements of types Settlement, Preappoved, Resch to Sett,\\\n",
    "                  Out of Mandate Had Settlement and null on the Earth portfolio, the corresponding number of applications, \\\n",
    "                  the initial settlement amount, discount amount and future instalments per asset class and DCA, for September 2020.',\n",
    "    'Report 6_2': 'Provide the distribution of the number of active and running settlements of types Settlement, Preappoved, Resch to Sett, \\\n",
    "                  Out of Mandate Had Settlement and null on the Earth portfolio based on their duration on the following bins: up to 6 months, \\\n",
    "                  7 to 12 months, 13 to 36 months, 37 to 72 months, 72 to 108 months and more than 109 months. The analysis should be \\\n",
    "                  performane per asset class and DCA, for September 2020.',\n",
    "    'Report 8_2': 'Create a report that shows the number of pending settlement applications (in status Review, Working, \\\n",
    "                  Quality Control, For approval and Approved) submitted up to the end of the previous month, their approved amount,\\\n",
    "                  the average pending days  and their entry principal and balance, for September 2020  per DCA and application type. \\\n",
    "                  The report should be produced on Earth portfolio.',\n",
    "    'Report 8_1': 'Create a report that shows the number of settlement applications submitted during the month, \\\n",
    "                  their approved amount and their entry principal and balance, for September 2020  per DCA and application type. \\\n",
    "                  The report should be produced on Earth portfolio.',\n",
    "    'Report 8_3': 'Create a report that shows the number of settlement applications approved during the month, their approved amount, \\\n",
    "                  the written off balance, the average days to approval, their average and median duration and their entry principal and balance, \\\n",
    "                  for September 2020 per DCA and application type. The report should be produced on Earth portfolio.',\n",
    "    'Report 8_4': 'Create a report that shows the number of settlement applications rejected during the month, their approved amount, \\\n",
    "                  the average days to rejection, their entry principal and entry balance, for September 2020 per DCA \\\n",
    "                  and application type. The report should be produced on Earth portfolio.'  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = ReportGenerator(description=data['Payment predictions 1'], model='sbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MEAS_APPL_AMT_PAYMENTS_TOTAL']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 71.42857142857143} precision\n",
      "{'Report 6_2': 83.33333333333334} recall\n",
      "{'Report 8_2': 60.0} precision\n",
      "{'Report 8_2': 75.0} recall\n",
      "{'Report 8_1': 75.0} precision\n",
      "{'Report 8_1': 100.0} recall\n",
      "{'Report 8_3': 71.42857142857143} precision\n",
      "{'Report 8_3': 71.42857142857143} recall\n",
      "{'Report 8_4': 60.0} precision\n",
      "{'Report 8_4': 60.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='sbert')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 28.57142857142857} precision\n",
      "{'Report 6_2': 33.33333333333333} recall\n",
      "{'Report 8_2': 40.0} precision\n",
      "{'Report 8_2': 50.0} recall\n",
      "{'Report 8_1': 50.0} precision\n",
      "{'Report 8_1': 66.66666666666666} recall\n",
      "{'Report 8_3': 57.14285714285714} precision\n",
      "{'Report 8_3': 57.14285714285714} recall\n",
      "{'Report 8_4': 60.0} precision\n",
      "{'Report 8_4': 60.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='bow')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 85.71428571428571} precision\n",
      "{'Report 6_2': 100.0} recall\n",
      "{'Report 8_2': 40.0} precision\n",
      "{'Report 8_2': 50.0} recall\n",
      "{'Report 8_1': 50.0} precision\n",
      "{'Report 8_1': 66.66666666666666} recall\n",
      "{'Report 8_3': 42.857142857142854} precision\n",
      "{'Report 8_3': 42.857142857142854} recall\n",
      "{'Report 8_4': 60.0} precision\n",
      "{'Report 8_4': 60.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='tfidf')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 0.0} precision\n",
      "{'Report 6_2': 0.0} recall\n",
      "{'Report 8_2': 60.0} precision\n",
      "{'Report 8_2': 75.0} recall\n",
      "{'Report 8_1': 75.0} precision\n",
      "{'Report 8_1': 100.0} recall\n",
      "{'Report 8_3': 100.0} precision\n",
      "{'Report 8_3': 100.0} recall\n",
      "{'Report 8_4': 60.0} precision\n",
      "{'Report 8_4': 60.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='bow_bigrams')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 100.0} precision\n",
      "{'Payment predictions 2': 50.0} recall\n",
      "{'Report 6_1': 25.0} precision\n",
      "{'Report 6_1': 25.0} recall\n",
      "{'Report 6_2': 57.14285714285714} precision\n",
      "{'Report 6_2': 66.66666666666666} recall\n",
      "{'Report 8_2': 20.0} precision\n",
      "{'Report 8_2': 25.0} recall\n",
      "{'Report 8_1': 25.0} precision\n",
      "{'Report 8_1': 33.33333333333333} recall\n",
      "{'Report 8_3': 14.285714285714285} precision\n",
      "{'Report 8_3': 14.285714285714285} recall\n",
      "{'Report 8_4': 20.0} precision\n",
      "{'Report 8_4': 20.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='tfidf_bigrams')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 0.0} precision\n",
      "{'Report 6_2': 0.0} recall\n",
      "{'Report 8_2': 60.0} precision\n",
      "{'Report 8_2': 75.0} recall\n",
      "{'Report 8_1': 75.0} precision\n",
      "{'Report 8_1': 100.0} recall\n",
      "{'Report 8_3': 100.0} precision\n",
      "{'Report 8_3': 100.0} recall\n",
      "{'Report 8_4': 60.0} precision\n",
      "{'Report 8_4': 60.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='bow_trigrams')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 0.0} precision\n",
      "{'Report 6_2': 0.0} recall\n",
      "{'Report 8_2': 60.0} precision\n",
      "{'Report 8_2': 75.0} recall\n",
      "{'Report 8_1': 75.0} precision\n",
      "{'Report 8_1': 100.0} recall\n",
      "{'Report 8_3': 100.0} precision\n",
      "{'Report 8_3': 100.0} recall\n",
      "{'Report 8_4': 60.0} precision\n",
      "{'Report 8_4': 60.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='tfidf_trigrams')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 83.33333333333334} precision\n",
      "{'Report 6_2': 83.33333333333334} recall\n",
      "{'Report 8_2': 40.0} precision\n",
      "{'Report 8_2': 50.0} recall\n",
      "{'Report 8_1': 50.0} precision\n",
      "{'Report 8_1': 66.66666666666666} recall\n",
      "{'Report 8_3': 42.857142857142854} precision\n",
      "{'Report 8_3': 42.857142857142854} recall\n",
      "{'Report 8_4': 80.0} precision\n",
      "{'Report 8_4': 80.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='word2vec-google-news-300')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 85.71428571428571} precision\n",
      "{'Report 6_2': 100.0} recall\n",
      "{'Report 8_2': 20.0} precision\n",
      "{'Report 8_2': 25.0} recall\n",
      "{'Report 8_1': 25.0} precision\n",
      "{'Report 8_1': 33.33333333333333} recall\n",
      "{'Report 8_3': 57.14285714285714} precision\n",
      "{'Report 8_3': 57.14285714285714} recall\n",
      "{'Report 8_4': 60.0} precision\n",
      "{'Report 8_4': 60.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='glove-wiki-gigaword-50')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 85.71428571428571} precision\n",
      "{'Report 6_2': 100.0} recall\n",
      "{'Report 8_2': 60.0} precision\n",
      "{'Report 8_2': 75.0} recall\n",
      "{'Report 8_1': 75.0} precision\n",
      "{'Report 8_1': 100.0} recall\n",
      "{'Report 8_3': 42.857142857142854} precision\n",
      "{'Report 8_3': 42.857142857142854} recall\n",
      "{'Report 8_4': 60.0} precision\n",
      "{'Report 8_4': 60.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='fasttext-wiki-news-subwords-300')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "{'Payment predictions 1': 0.0} precision\n",
      "{'Payment predictions 1': 0.0} recall\n",
      "{'Payment predictions 2': 0.0} precision\n",
      "{'Payment predictions 2': 0.0} recall\n",
      "{'Report 6_1': 75.0} precision\n",
      "{'Report 6_1': 75.0} recall\n",
      "{'Report 6_2': 85.71428571428571} precision\n",
      "{'Report 6_2': 100.0} recall\n",
      "{'Report 8_2': 0.0} precision\n",
      "{'Report 8_2': 0.0} recall\n",
      "{'Report 8_1': 0.0} precision\n",
      "{'Report 8_1': 0.0} recall\n",
      "{'Report 8_3': 28.57142857142857} precision\n",
      "{'Report 8_3': 28.57142857142857} recall\n",
      "{'Report 8_4': 40.0} precision\n",
      "{'Report 8_4': 40.0} recall\n"
     ]
    }
   ],
   "source": [
    "for report, description in data.items():\n",
    "    rg = ReportGenerator(description=description, model='glove-wiki-gigaword-300')\n",
    "    print(rg.evaluation(metric='precision', report=report), 'precision')\n",
    "    print(rg.evaluation(metric='recall', report=report), 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DCA'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg = ReportGenerator(description=data['Report 8_4'], model='bow')\n",
    "lov = rg.get_unique_values_per_type()['MT_LOV']\n",
    "lov\n",
    "lov[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mDoes the word/phrase application type . act as a grouping or a filtering predicate in the following description?:Create a report that shows the number of pending settlement applications (in status Review, Working,                   Quality Control, For approval and Approved) submitted up to the end of the previous month, their approved amount,                  the average pending days  and their entry principal and balance, for September 2020  per DCA and application type.                   The report should be produced on Earth portfolio..                        If it is a grouping predicate return 1. If it is a filtering predicate return 0.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.get_operator_type_for_lov(lov=lov[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approved amount': ['MEAS_APPL_AMT_APPROVED'],\n",
       " 'entry principal': ['MEAS_APPL_AMT_APPROVED'],\n",
       " 'entry balance': ['MEAS_APLL_AMT_WRITEOFF']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.get_similar_vectors_in_index_per_value(type='MT_MONEY', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAre there one or more aggregation functions, such as average, sum, median etc, that should be applied to this quantity                       approved amount according to this description?: Create a report that shows the number of settlement applications rejected during the month, their approved amount,                   the average days to rejection, their entry principal and entry balance, for September 2020 per DCA                   and application type. The report should be produced on Earth portfolio.. Provide only the name of the function(s).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAre there one or more aggregation functions, such as average, sum, median etc, that should be applied to this quantity                       entry principal according to this description?: Create a report that shows the number of settlement applications rejected during the month, their approved amount,                   the average days to rejection, their entry principal and entry balance, for September 2020 per DCA                   and application type. The report should be produced on Earth portfolio.. Provide only the name of the function(s).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAre there one or more aggregation functions, such as average, sum, median etc, that should be applied to this quantity                       entry balance according to this description?: Create a report that shows the number of settlement applications rejected during the month, their approved amount,                   the average days to rejection, their entry principal and entry balance, for September 2020 per DCA                   and application type. The report should be produced on Earth portfolio.. Provide only the name of the function(s).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAre there one or more aggregation functions, such as average, sum, median etc, that should be applied to this quantity                       number according to this description?: Create a report that shows the number of settlement applications rejected during the month, their approved amount,                   the average days to rejection, their entry principal and entry balance, for September 2020 per DCA                   and application type. The report should be produced on Earth portfolio.. Provide only the name of the function(s).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAre there one or more aggregation functions, such as average, sum, median etc, that should be applied to this quantity                       days rejection according to this description?: Create a report that shows the number of settlement applications rejected during the month, their approved amount,                   the average days to rejection, their entry principal and entry balance, for September 2020 per DCA                   and application type. The report should be produced on Earth portfolio.. Provide only the name of the function(s).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MEAS_APPL_AMT_APPROVED': '\\n\\nAverage, Sum, Median',\n",
       " 'MEAS_APLL_AMT_WRITEOFF': '\\n\\nAverage, Sum',\n",
       " 'MEAS_ACCH_SNAPNUM': '\\n\\nAverage, Sum, Median',\n",
       " 'MEAS_APLL_DAYS_REJECTED': '\\n\\nAverage'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg.get_column_agg_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
